{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IMC_P3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9P2KRTAQl6L",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "#**PRÁCTICA 3: REDES NEURONALES DE FUNCIONES DE BASE RADIAL**\n",
        "\n",
        "##Redes RBF\n",
        "\n",
        "Las redes neuronales de funciones o base radial, o funciones RBF son redes basadas en una aproximación local.\n",
        "\n",
        "Para esta aproximación local se utiliza un algoritmo de clustering (K-Medias) con el cual se localizarán las coordenadas de los centroides, los cuales serán los centros de las RBF.\n",
        "\n",
        "Una vez obtenidos los centroides, se ha de calcular el radio de cada uno de estos. En nuestra práctica, este radio se calcula mediante:\n",
        "\n",
        "![Fórmula radio centroides](https://drive.google.com/uc?id=1zu5x3q4dJfE0kBz56NC5fG5bRfe36KMI)\n",
        "\n",
        "Tras esto, se han de ajustar los pesos de la capa de salida. Esto variará si estamos en un problema de clasificación o de regresión:\n",
        "\n",
        "\n",
        "*   En problemas de regresión utilizaremos la Pseudo-Inversa de Moore-Penrose:\n",
        "\n",
        "  ![texto alternativo](https://drive.google.com/uc?id=1w87TlpRGLcertUXyC-lPTSF9qpnJUE1-)\n",
        "\n",
        "  Donde **beta** es la matriz de parámetros, **R** la pseudo-inversa de la matriz de salidas y **Y** la matriz con las salidas deseadas. Una vez obtenida la matriz **beta**, la matriz de salidas se obtiene:\n",
        "\n",
        "  ![texto alternativo](https://drive.google.com/uc?id=1Odct9IRVAH6C_kXwz93d0aEdfRFGRHIC)\n",
        "\n",
        "\n",
        "*   Un problemas de clasificación se utlizará la regresión logística. Se pueden usar 2 tipos de regularización:\n",
        "  - L1(poda más variable):\n",
        "\n",
        "      ![texto alternativo](https://drive.google.com/uc?id=1NedFOiCxprmQzDK9vFztQxEkLO_XakoV)\n",
        "\n",
        "    - L2(pesos más pequeños):\n",
        "\n",
        "      ![texto alternativo](https://drive.google.com/uc?id=1kxcHYwHYgIWfnkuVa35dsraytu8tOBan)\n",
        "\n",
        "\n",
        "##Experimentos y análisis de los resultados\n",
        "\n",
        "###Bases de datos utilizadas\n",
        "\n",
        "\n",
        "*   Base de datos SIN\n",
        "      - Contiene diferentes valores de entrada para la salida de la función seno.\n",
        "      - Posee 1 valor de entrada.\n",
        "      - Posee 1 valor de salida.\n",
        "      - Formada por 120 patrones de entrenamiento.\n",
        "      - Formada por 41 patrones de test.\n",
        "\n",
        "*   Base de datos QUAKE\n",
        "      - Contiene diferentes parámetros interesantes de un terremoto (entradas) para así estimar la fuerza de este (salida).\n",
        "      - Posee 3 valores de entrada.\n",
        "      - Posee 1 valor de salida.\n",
        "      - Formada por 1633 patrones de entrenamiento.\n",
        "      - Formada por 546 patrones de test.\n",
        "\n",
        "*   Base de datos PARKINSONS\n",
        "      - Contiene diferentes parámetros clínicos y biométricos de personas con parkinson como entradas y, como salida, el valor motor y UPDRS.\n",
        "      - Posee 19 valores de entrada.\n",
        "      - Posee 2 valores de salida.\n",
        "      - Formada por 4406 patrones de entrenamiento.\n",
        "      - Formada por 1469 patrones de test.\n",
        "\n",
        "*   Base de datos VOTE\n",
        "      - Contiene el diferente número de votos a los diferentes candidatos al congreso de EE.UU.\n",
        "      - Posee 16 valores de entrada (categóricos).\n",
        "      - Posee 2 valores de salida.\n",
        "      - Formada por 326 patrones de entrenamiento.\n",
        "      - Formada por 109 patrones de test.\n",
        "\n",
        "*   Base de datos NOMNIST\n",
        "      - Contiene informacion de los pixeles de diferentes imagenes de letras (de la A a la F).\n",
        "      - Posee 784 valores de entrada.\n",
        "      - Posee 6 valor de salida (Uno para cada letra de la A a la F).\n",
        "      - Formada por 900 patrones de entrenamiento.\n",
        "      - Formada por 300 patrones de test.\n",
        "\n",
        "\n",
        "###Parámetros considerados\n",
        "\n",
        "\n",
        "\n",
        "*   Ratio RBF (-r): Porcentaje del número de patrones de la base de datos que se emplearán como neuronas RBF. Expresado en tanto por uno.\n",
        "\n",
        "*   Regularización (-l): Indica el tipo de regularización que se utilizará en los problemas de clasificaciñon (L1 o L2).\n",
        "\n",
        "*   Parámetro *eta* (-e): Parámetro con el que se indica la importancia que le daremos a la regularización.\n",
        "\n",
        "*   Salidas (-o): Número de columnas de salida que posee la base de datos.\n",
        "\n",
        "*   Clasificación (-c): Booleano que indica si el problema a predecir es de regresión (false) o clasificación (true).\n",
        "\n",
        "###Experimentación y análisis de los resultados\n",
        "\n",
        "####Base de datos SIN\n",
        "\n",
        "Esta base de datos, junto a las bases de datos Quake y Parkinsons se usan para entrenar la red neuronal RBF para predecir mediante regresión, es decir, predicen un valor que es contínuo.\n",
        "\n",
        "Para estas bases de datos se ha extraído su mse, con su respectiva desviación típica, para test y entrenamiento.\n",
        "\n",
        "El primer experimento realizado ha sido simplemente varias el número de neuronas en capas ocultas (parámetro ratio_rbf), para así encontrar la mejor arquitectura.\n",
        "\n",
        "```\n",
        "#Simplemente hay que cambiar el valor introducido en 'ratio_rbf'\n",
        "def entrenar_rbf_total(train_file, test_file, classification, ratio_rbf, l2, eta, outputs, model_file, pred)\n",
        "```\n",
        "\n",
        "Los resultados de este experimentro han sido los siguientes:\n",
        "\n",
        "![Experimento SIN 1](https://drive.google.com/uc?id=1B6aVH-xUqSq_Z-8BBG3xz_8v_ZLPe2yL)\n",
        "\n",
        "Como podemos observar, en esta base de datos el mejor resultado de obtiene utilizando un 5% de los patrones como centroides. Como podemos observar, la media de entrenamiento si que es mejor con un 50% de neuronas, en cambio, la de test la mejor se consigue en 5%, esto se puede deber a que al ser una base de datos pequeña y más o menos simple, al añadir un exceso de patrones como centroides, se tiende a sobreentrenar, dando así peores resultados en test.\n",
        "\n",
        "El siguiente de los experimentos ha sido cambiar la inicialización del algoritmo Kmeans a 'Kmeans++'. Este método de inicialización selecciona los centroides de los clusters de forma que se produzca una convergencia más rápida.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# Cambiamos 'centroides' por 'Kmeans++'\n",
        "kmedias = KMeans(n_clusters = num_rbf, init = centroides, n_init = 1, max_iter = 500)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "Los resultados de este segundo experimento han sido los siguientes:\n",
        "\n",
        "![Experimento SIN 2](https://drive.google.com/uc?id=1VtZiIfkv-TtlgiC8YQKZ7HincRJnS3Z7)\n",
        "\n",
        "Podemos que se consigue cierta mejora en la media de test, sin embargo, la desviación típica aumenta, por lo que se puede considerar que los resultados no mejoran.\n",
        "\n",
        "####Base de datos Quake\n",
        "\n",
        "Para esta segunda base de datos de regresión se ha procedido a realizar los mismos experimentos que con la anterior.\n",
        "\n",
        "Primero se ha variado el porcentaje de patrones que se usarán como centroides y, por ende, el número de capas ocultas de la red. El resultado es el siguiente:\n",
        "\n",
        "![Experimento Quake 1](https://drive.google.com/uc?id=1eikrGMSFgXd09dwIc1Av1suwbWCTO2Hi)\n",
        "\n",
        "El resultado es similar a la anterior, la media de entrenamiento mejora conforme aumentamos el porcentaje RBF mientras que la media de test empeora, obteniendo la mejor con un 5%. Esto se puede deber también a motivos similares a la anterior base de datos.\n",
        "\n",
        "Con esta mejor configuración, se ha pasado a cambiar la inicialización del algoritmo a 'Kmeans++'. Los resultados han sido:\n",
        "\n",
        "![Experimento Quake 2](https://drive.google.com/uc?id=1-IljQBKGJYguVn025sMLFp9z_X9GNus4)\n",
        "\n",
        "Se obtienen unos resultados prácticamente semejantes a la obtenida con la anterior inicialización, ya que mejora algo la media de test pero empeora la desviacion típica.\n",
        "\n",
        "\n",
        "####Base de datos Parkinsons\n",
        "\n",
        "Para esta tercera y última base de datos de regresión se han realizado también los mismos experimentos que en las anteriores.\n",
        "\n",
        "Los resultados para el primer experimento han sido:\n",
        "\n",
        "![Experimento Parkinsons 1](https://drive.google.com/uc?id=1GS60dl42UXODNVsHcAtH3d1hsskG_uBY)\n",
        "\n",
        "En esta ocasión el mejor resultado para test se obtiene con un 15% de patrones, aunque sigue la dinámica de las anteriores bases de datos, en la que la media de entrenamiento si que continúa mejorando, siendo la mejor al 50%. Otro detalle a observar es que en esta ocasión, la desviación típica en test va aumentando considerablemente conforme aumentamos el porcentaje rbf.\n",
        "\n",
        "Cambiando el modo de inicialización del algoritmo a 'Kmeans++', los resultados han sido los siguientes:\n",
        "\n",
        "![Experimento Parkinsons 2](https://drive.google.com/uc?id=16qpu2Az3V0jn1Jl5NIHeSOUdTYj79FQK)\n",
        "\n",
        "####Base de datos Vote\n",
        "\n",
        "Tanto esta base de datos como la siguiente, se tratan de bases de datos de clasificación, es decir, poseen dos o más clases a predecir. En este tipo de bases de datos, además de los experimentos realizados para las de regresión, se han aplicado 2 experimentos más.\n",
        "\n",
        "El primero de los experimentos ha sido el mismo que para las anteriores, el cual consiste en varias el porcentaje RBF. Los resultados han sido los siguientes:\n",
        "\n",
        "![Experimento vote 1](https://drive.google.com/uc?id=1Awr90sUN39hMoaEffI-o-Ui5o5cekAIr)\n",
        "\n",
        "Observamos que en esta primera base de datos, los mejores resultados con respecto al CCR se obtienen con 5% - 25%, ya que en ambos se obtiene el mismo porcentaje.\n",
        "\n",
        "Una vez obtenido este mejor porcentaje RBF, se ha procedido ha cambiar el tipo de regularización (L1 y L2) y, en cada uno de estos, variando el valor de eta.\n",
        "Los resultados han sido los siguientes\n",
        "\n",
        "Con L1:\n",
        "\n",
        "![Experimento vote 2](https://drive.google.com/uc?id=1AYMYp0MH2fyxg7cowh2vRVZbEU2Yp0tI)\n",
        "\n",
        "Con L2:\n",
        "\n",
        "![Experimento vote 2](https://drive.google.com/uc?id=1DXeEchFMGX-0Vf7hm7f0nAFAlEUYWZLp)\n",
        "\n",
        "Observamos que el mejor resultado se obtiene con un eta de 0.0001 y modo de regularización L2, obteniendo alrededor de 97% de CCR de test. Con este resultado también obtenemos la mejor media de test. \n",
        "\n",
        "Se mejora con respecto al primer experimento.\n",
        "\n",
        "Con respecto al tipo de regularización, se ha procedido a comparar el número de coeficientes para cada valor de eta:\n",
        "\n",
        "![Coeficientes Vote](https://drive.google.com/uc?id=1c2mTY94-e-vYIDiiz7D83-SsG2nd9KZU)\n",
        "\n",
        "Como podemos ver, al usar regularización L1 aparece un menor número de coeficientes que al usar regularización L2. Según hemos visto, la regularización L1 tiende a eliminar más variables, lo cual explica el menor número de coeficientes respecto a la regularización L2. La diferencia se vuelve menor a medida que se reduce la importancia de la regularización (valores más bajos de eta).\n",
        "\n",
        "El tercer experimento ha sido cambiar el método de inicialización del algoritmo a 'Kmeans++'. Los resultados han sido:\n",
        "\n",
        "![Experimento Vote 3](https://drive.google.com/uc?id=1gCJZmp98kDIrNxPM_TliU2Glt0fJbppu)\n",
        "\n",
        "Los resultados empeoran con este método de inicialización con respecto al experimento anterior, aunque mejora respecto al primero.\n",
        "\n",
        "El último de los experimentos ha sido el de tratar estas bases de datos como regresión,  es decir, intentar predecir el valor de la variable de salida como si se tratase de una variable continua, y obtener el CCR redondeando al entero más cercano.\n",
        "\n",
        "\n",
        "Los resultados han sido los siguientes:\n",
        "\n",
        "![Experimento Vote 4](https://drive.google.com/uc?id=1zrxbfGJMzWAcUpKkBYJTDSqf-CSYdcMa)\n",
        "\n",
        "Los resultados tampoco mejoran con este experimento con respecto a los mejores obtenidos.\n",
        "\n",
        "####Base de datos Nomnist\n",
        "\n",
        "Para esta base de datos se han procedido a realizar los mismos experimentos que para la anterior.\n",
        "\n",
        "Primero se ha probado con distintos porcentajes rbf, el resultado ha sido el siguiente:\n",
        "\n",
        "![Experimento Nomnist 1](https://drive.google.com/uc?id=17lrk0H1xa9onKUPwDZPVIIQBRmeKYT1q)\n",
        "\n",
        "En esta ocasión obtenemos los mejores resultados con un 5% de patrones. Con esta configuración, hemos variado el tipo de regularización y el parámetro eta. Los resultados obtenidos han sido:\n",
        "\n",
        "Con L1:\n",
        "\n",
        "![Experimento Nomnist 2](https://drive.google.com/uc?id=1x2BvHqbNjY2Uw_oQPdlyjXk9a2Tt1_Sz)\n",
        "\n",
        "Con L2:\n",
        "\n",
        "![Experimento Nomnist 2](https://drive.google.com/uc?id=1dwek-ftsTXTUIYIbyHkY61CfX-pPonLh)\n",
        "\n",
        "Para esta base de datos, el mejor resultado, 90.97%, con el tipo de regularización L1 y un eta de 0.01\n",
        "\n",
        "Con respecto al tipo de regularización, se ha procedido a comparar el número de coeficientes para cada valor de eta:\n",
        "\n",
        "![Coeficientes Vote](https://drive.google.com/uc?id=1niGTTiZsv1ahlclcVa_1IADOTsPBMIZU)\n",
        "\n",
        "El siguiente de los experimentos es el modificar el método de inicialización del algoritmo, los resultados han sido:\n",
        "\n",
        "![Experimento Nomnist 3](https://drive.google.com/uc?id=1U1Oc_NGTBTnfawP9FU3rHcp9BTYR9Gw9)\n",
        "\n",
        "Los resultados no mejoran con este tipo de inicialización.\n",
        "\n",
        "Por último, se ha tratado este problema como un problema de clasificación, los resultados han sido:\n",
        "\n",
        "![Experimento Nomnist 4](https://drive.google.com/uc?id=15Y9Hcm4y3puy5wBEbOHtMDyGFAGL9q-d)\n",
        "\n",
        "Como podemos observar, los resultados son bastante malos; esto tiene sentido, ya que al trabajar con problemas de clasificación no tiene por qué existir ningún tipo de relación numérica entre los valores de las variables independientes y el de la etiqueta de clase, que se puede asignar de forma totalmente arbitraria.\n",
        "\n",
        "Por último, para el mejor resultado obtenido entre todos los experimentos (L1, 0,001 eta y 5% RBF) se ha calculado la matriz de confusión.\n",
        "\n",
        "\n",
        "![Matriz de confusión](https://drive.google.com/uc?id=1y9hy2_aT-Klm_qii0NUFrULo0dctD-50)\n",
        "\n",
        "La matriz de confusión se ha calculado mediante el siguiente código:\n",
        "\n",
        "```\n",
        "        confusion = np.zeros((6, 6), dtype=int)\n",
        "\n",
        "        print(\"\")\n",
        "\n",
        "        for i in range(test_outputs.shape[0]):\n",
        "            real = np.argmax(test_outputs[i])\n",
        "            pred = np.argmax(test_outputs_pred[i])\n",
        "\n",
        "            print(\"Patron %d. Clase real: %d; Clase predicha: %d\" % (i, real, pred))\n",
        "            confusion[real, pred] = confusion[real, pred] + 1\n",
        "\n",
        "        print(\"\")\n",
        "        print(\"Matriz de confusión: \")\n",
        "        print(\"\")\n",
        "        print(confusion)\n",
        "        print(\"\")\n",
        "```\n",
        "\n",
        "Estudiando los errores cometidos por la red, observamos los siquientes:\n",
        "\n",
        "\n",
        "\n",
        "*   A mal clasificadas:\n",
        "\n",
        "      Clasificada como D\n",
        "\n",
        "      ![Matriz de confusión](https://drive.google.com/uc?id=1FVhIilBEzPEfktf5aSSd7uWVqQJaIn9N)\n",
        "\n",
        "      Clasificada como F\n",
        "\n",
        "      ![Matriz de confusión](https://drive.google.com/uc?id=1HgnwdecCxYFYZofYXR2cy-H1LteX1MVD)\n",
        "\n",
        "      Observamos que la que se ha clasificado como F, si que varía con respecto a la forma normal de una A, sin embargo, la otra no tiene apenas variación. La primera podría considerarse como una D girada 90º.\n",
        "\n",
        "\n",
        "*   B mal clasificadas:\n",
        "\n",
        "      Clasificada como A\n",
        "\n",
        "      ![Matriz de confusión](https://drive.google.com/uc?id=18Ef4Nh1fzkEN-TQbomEp9EBQHyII1Q-J)\n",
        "\n",
        "      Clasificada como D\n",
        "\n",
        "      ![Matriz de confusión](https://drive.google.com/uc?id=12n2f-NcxFSaeK1Xif_I-SOwaEsgujmZL)\n",
        "\n",
        "\n",
        "*   C mal clasificadas:\n",
        "\n",
        "      Clasificada como E\n",
        "\n",
        "      ![Matriz de confusión](https://drive.google.com/uc?id=1xy8jBGA8eQC1bx4OXKJ6sO3jcRf3TWEn)\n",
        "\n",
        "      Clasificada como F\n",
        "\n",
        "      ![Matriz de confusión](https://drive.google.com/uc?id=1biZxIs41-SV2VBakGNXzUeGZQz40jqeG)\n",
        "\n",
        "\n",
        "*   D mal clasificadas:\n",
        "\n",
        "      Clasificada como B\n",
        "\n",
        "      ![Matriz de confusión](https://drive.google.com/uc?id=1taRUNdd1XEgapZLZgpNKWxS7lY0Txgjf)\n",
        "\n",
        "      Clasificada como D\n",
        "\n",
        "      ![Matriz de confusión](https://drive.google.com/uc?id=1tIvRzotW05e99acIyZ9tv9NyS-jx90ki)\n",
        "\n",
        "      Estos se pueden deber a los parecidos que existen entre la B y la D.\n",
        "\n",
        "\n",
        "*   E mal clasificadas:\n",
        "\n",
        "      Clasificada como C\n",
        "\n",
        "      ![Matriz de confusión](https://drive.google.com/uc?id=1HpHF4n8iiyqp7XyK9_Zey6ebIT9Kzu2D)\n",
        "\n",
        "      Clasificada como F\n",
        "\n",
        "      ![Matriz de confusión](https://drive.google.com/uc?id=1RR1PsVfGCLmLh60lv3vV-7c8pXn6oscD)\n",
        "\n",
        "      Las letras E y F poseen bastante parecido.\n",
        "\n",
        "\n",
        "*   F mal clasificadas:\n",
        "\n",
        "      Clasificada como E\n",
        "\n",
        "      ![Matriz de confusión](https://drive.google.com/uc?id=1BiqMuBgXBydl6qCj-vmRblbxQ0Gua7LK)\n",
        "\n",
        "      Esto también se puede deber a los parecidos que hay entre la E y la F.\n",
        "\n",
        "Como conclusión, cabe destacar que los tiempo para la obtención de cada uno de los resultados ha sido cerca de 10-12 veces más rápido que en las prácticas anteriores. En esta práctica se tardaban en obtenes una ejecución del código alrededor de 2 minutos, mientras que en las anteriores podía llegar hasta los 15-20 minutos.\n",
        "\n",
        "##Código\n",
        "\n",
        "A continuación se procede a mostrar el código utilizado.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cj5glHyejbgu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    #from google.colab import files\n",
        "    #files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ii5rxjCK_E-Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Wed Oct 28 12:37:04 2016\n",
        "\n",
        "@author: pagutierrez\n",
        "\"\"\"\n",
        "\n",
        "# TODO Incluir todos los import necesarios\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "import click\n",
        "import pandas as pd #No lo utilizamos, ya que mediante Google Colab no es posible\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from scipy.spatial.distance import pdist, squareform"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSZHV6OrL6s8",
        "colab_type": "text"
      },
      "source": [
        "Imports necesarios para la ejecución de la práctica. La mayoria pertenecientes a Scikit Learn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2vE8oH3_GVv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"@click.command()\n",
        "@click.option('--train_file', '-t', default=None, required=False,\n",
        "              help=u'Fichero con los datos de entrenamiento.')\n",
        "\n",
        "# TODO incluir el resto de parámetros...\n",
        "\n",
        "@click.option('--test_file', '-T', default=\"\", required=True,\n",
        "              help=u'Fichero con los datos de test.')\n",
        "@click.option('--classification', '-c', is_flag=True, default=False, show_default=True,\n",
        "              help=u'Activar para problemas de clasificaciÃ³n.')\n",
        "@click.option('--ratio_rbf', '-r', default=0.1, show_default=True,\n",
        "              help=u'Ratio (en tanto por uno) de neuronas RBF con respecto al total de patrones.')\n",
        "@click.option('--l2', '-l', is_flag=True, default=False, show_default=True,\n",
        "              help=u'Activar para utilizar la regularizaciÃ³n de L2 en lugar de la regularización de L1 (regresiÃ³n logÃ­stica).')\n",
        "@click.option('--eta', '-e', default=0.01, show_default=True,\n",
        "              help=u'Valor del parÃ¡metro de regularización para la regresión logí­stica.')\n",
        "@click.option('--outputs', '-o', default=1, show_default=True,\n",
        "              help=u'Número de variables que se tomarÃ¡n como salidas.')\n",
        "@click.option('--model_file', '-m', default=\"\", show_default=True,\n",
        "              help=u'Fichero en el que se guardará o desde el que se cargará el modelo (si existe el flag p).') # KAGGLE\n",
        "@click.option('--pred', '-p', is_flag=True, default=False, show_default=True,\n",
        "              help=u'Activar el modo de predicción.') \"\"\"\n",
        "\n",
        "# KAGGLE\n",
        "def entrenar_rbf_total(train_file, test_file, classification, ratio_rbf, l2, eta, outputs, model_file, pred):\n",
        "    \"\"\" Modelo de aprendizaje supervisado mediante red neuronal de tipo RBF.\n",
        "        Ejecución de 5 semillas.\n",
        "    \"\"\"\n",
        "\n",
        "    if not pred:\n",
        "\n",
        "        if train_file is None:\n",
        "            print(\"No se ha especificado el conjunto de entrenamiento (-t)\")\n",
        "            return\n",
        "\n",
        "        train_mses = np.empty(5)\n",
        "        train_ccrs = np.empty(5)\n",
        "        test_mses = np.empty(5)\n",
        "        test_ccrs = np.empty(5)\n",
        "\n",
        "        for s in range(1,6,1):\n",
        "            \"\"\"print(\"-----------\")\n",
        "            print(\"Semilla: %d\" % s)\n",
        "            print(\"-----------\")\"\"\"\n",
        "            np.random.seed(s)\n",
        "            train_mses[s-1], test_mses[s-1], train_ccrs[s-1], test_ccrs[s-1] = \\\n",
        "                entrenar_rbf(train_file, test_file, classification, ratio_rbf, l2, eta, outputs, \\\n",
        "                             model_file and \"{}/{}.pickle\".format(model_file, s) or \"\")\n",
        "            print(\"MSE de entrenamiento: %f\" % train_mses[s-1])\n",
        "            print(\"MSE de test: %f\" % test_mses[s-1])\n",
        "            if classification:\n",
        "                print(\"CCR de entrenamiento: %.2f%%\" % train_ccrs[s-1])\n",
        "                print(\"CCR de test: %.2f%%\" % test_ccrs[s-1])\n",
        "\n",
        "        print(\"*********************\")\n",
        "        print(\"Resumen de resultados Clasificación como regresión/ Base de datos: \", train_file, \"/ Capas ocultas: \", ratio_rbf, \"/ Eta: \", eta, \"/ L2: \", l2)\n",
        "        print(\"*********************\")\n",
        "        print(\"MSE de entrenamiento: %lf +- %lf\" % (np.mean(train_mses), np.std(train_mses)))\n",
        "        print(\"MSE de test: %lf +- %lf\" % (np.mean(test_mses), np.std(test_mses)))\n",
        "\n",
        "        print(\"CCR de entrenamiento: %.2f%% +- %.4f%%\" % (np.mean(train_ccrs), np.std(train_ccrs)))\n",
        "        print(\"CCR de test: %.2f%% +- %.4f%%\" % (np.mean(test_ccrs), np.std(test_ccrs))) \n",
        "\n",
        "    else:\n",
        "        # KAGGLE\n",
        "        if model_file is None:\n",
        "            print(\"No se ha indicado un fichero que contenga el modelo (-m).\")\n",
        "            return\n",
        "\n",
        "        # Obtener predicciones para el conjunto de test\n",
        "        predictions = predict(test_file, model_file)\n",
        "\n",
        "        # Imprimir las predicciones en formato csv\n",
        "        print(\"Id,Category\")\n",
        "        for prediction, index in zip(predictions, range(len(predictions))):\n",
        "            s = \"\"\n",
        "            s += str(index)\n",
        "\n",
        "            if isinstance(prediction, np.ndarray):\n",
        "                for output in prediction:\n",
        "                    s += \",{}\".format(output)\n",
        "            else:\n",
        "                s += \",{}\".format(int(prediction))\n",
        "\n",
        "            print(s)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUKuY4mqMKKk",
        "colab_type": "text"
      },
      "source": [
        "Función principal en la que ejecutamos las iteraciones necesarias el algoritmo y calculamos los errores porcentajes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bpp9N3eF_Giv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def entrenar_rbf(train_file, test_file, classification, ratio_rbf, l2, eta, outputs, model_file=\"\"):\n",
        "    \"\"\" Modelo de aprendizaje supervisado mediante red neuronal de tipo RBF.\n",
        "        Una única ejecución.\n",
        "        Recibe los siguientes parámetros:\n",
        "            - train_file: nombre del fichero de entrenamiento.\n",
        "            - test_file: nombre del fichero de test.\n",
        "            - classification: True si el problema es de clasificacion.\n",
        "            - ratio_rbf: Ratio (en tanto por uno) de neuronas RBF con \n",
        "              respecto al total de patrones.\n",
        "            - l2: True si queremos utilizar L2 para la Regresión Logística. \n",
        "              False si queremos usar L1 (para regresión logística).\n",
        "            - eta: valor del parámetro de regularización para la Regresión \n",
        "              Logística.\n",
        "            - outputs: número de variables que se tomarán como salidas \n",
        "              (todas al final de la matriz).\n",
        "        Devuelve:\n",
        "            - train_mse: Error de tipo Mean Squared Error en entrenamiento. \n",
        "              En el caso de clasificación, calcularemos el MSE de las \n",
        "              probabilidades predichas frente a las objetivo.\n",
        "            - test_mse: Error de tipo Mean Squared Error en test. \n",
        "              En el caso de clasificación, calcularemos el MSE de las \n",
        "              probabilidades predichas frente a las objetivo.\n",
        "            - train_ccr: Error de clasificación en entrenamiento. \n",
        "              En el caso de regresión, devolvemos un cero.\n",
        "            - test_ccr: Error de clasificación en test. \n",
        "              En el caso de regresión, devolvemos un cero.\n",
        "    \"\"\"\n",
        "    train_inputs, train_outputs, test_inputs, test_outputs = lectura_datos(train_file, \n",
        "                                                                           test_file,\n",
        "                                                                           outputs)\n",
        "\n",
        "    #TODO: Obtener num_rbf a partir de ratio_rbf\n",
        "\n",
        "    num_rbf = int(train_inputs.shape[0] * ratio_rbf)\n",
        "\n",
        "    #print(\"Número de RBFs utilizadas: %d\" %(num_rbf))\n",
        "\n",
        "    kmedias, distancias, centros = clustering(classification, train_inputs, \n",
        "                                              train_outputs, num_rbf)\n",
        "    \n",
        "    radios = calcular_radios(centros, num_rbf)\n",
        "    matriz_r = calcular_matriz_r(distancias, radios)\n",
        "\n",
        "    if not classification:\n",
        "        coeficientes = invertir_matriz_regresion(matriz_r, train_outputs)\n",
        "    else:\n",
        "        logreg = logreg_clasificacion(matriz_r, train_outputs, eta, l2)\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    TODO: Calcular las distancias de los centroides a los patrones de test\n",
        "          y la matriz R de test\n",
        "    \"\"\"\n",
        "\n",
        "    distancias_test = kmedias.transform(test_inputs)\n",
        "    matriz_r_test = calcular_matriz_r(distancias_test, radios)\n",
        "\n",
        "    # # # # KAGGLE # # # #\n",
        "    if model_file != \"\":\n",
        "        save_obj = {\n",
        "            'classification': classification,\n",
        "            'radios': radios,\n",
        "            'kmedias': kmedias\n",
        "        }\n",
        "        if not classification:\n",
        "            save_obj['coeficientes'] = coeficientes\n",
        "        else:\n",
        "            save_obj['logreg'] = logreg\n",
        "\n",
        "        dir = os.path.dirname(model_file)\n",
        "        if not os.path.isdir(dir):\n",
        "            os.makedirs(dir)\n",
        "\n",
        "        with open(model_file, 'wb') as f:\n",
        "            pickle.dump(save_obj, f)\n",
        "\n",
        "    # # # # # # # # # # #\n",
        "\n",
        "    if not classification:\n",
        "        \"\"\"\n",
        "        TODO: Obtener las predicciones de entrenamiento y de test y calcular\n",
        "              el MSE\n",
        "        \"\"\"\n",
        "\n",
        "        train_outputs_est = matriz_r.dot(coeficientes) #np.dot(matriz_r, coeficientes)\n",
        "        test_outputs_est = matriz_r_test.dot(coeficientes)\n",
        "\n",
        "        #Se redondea la salida al entero más próximo. Usar solo al tratar de resolver un problema\n",
        "        #de clasificación como regresión.\n",
        "\n",
        "        #train_outputs_est = np.around(train_outputs_est)\n",
        "        #test_outputs_est = np.around(test_outputs_est)\n",
        "\n",
        "        train_mse = mean_squared_error(train_outputs, train_outputs_est)\n",
        "        test_mse = mean_squared_error(test_outputs, test_outputs_est)\n",
        "\n",
        "        train_ccr = 0\n",
        "        test_ccr = 0\n",
        "\n",
        "        \n",
        "        #Calculo del CCR al tratar de resolver un problema de clasificación como regresión.\n",
        "\n",
        "        #train_ccr = accuracy_score(train_outputs, train_outputs_est) * 100\n",
        "        #test_ccr = accuracy_score(test_outputs, test_outputs_est) * 100\n",
        "\n",
        "    else:\n",
        "        \"\"\"\n",
        "        TODO: Obtener las predicciones de entrenamiento y de test y calcular\n",
        "              el CCR. Calcular también el MSE, comparando las probabilidades \n",
        "              obtenidas y las probabilidades objetivo\n",
        "        \"\"\"\n",
        "\n",
        "        # Calculo de las probabilidades para cada clase.\n",
        "        train_outputs_pred = logreg.predict_proba(matriz_r)\n",
        "        test_outputs_pred = logreg.predict_proba(matriz_r_test)\n",
        "\n",
        "        # Calculo del porcentaje de acierto.\n",
        "        train_ccr = logreg.score(matriz_r, train_outputs) * 100\n",
        "        test_ccr = logreg.score(matriz_r_test, test_outputs) * 100\n",
        "\n",
        "\n",
        "        # Convertir las salidas en vectores one-hot.\n",
        "        enc = OneHotEncoder(sparse=False, categories='auto')\n",
        "        train_outputs = enc.fit_transform(train_outputs).astype(int)\n",
        "        test_outputs = enc.fit_transform(test_outputs).astype(int)\n",
        "\n",
        "        # Calculamos MSE\n",
        "        train_mse = mean_squared_error(train_outputs, train_outputs_pred)\n",
        "        test_mse = mean_squared_error(test_outputs, test_outputs_pred)\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        Matriz de Confusión\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"\n",
        "        confusion = np.zeros((6, 6), dtype=int)\n",
        "\n",
        "        print(\"\")\n",
        "\n",
        "        for i in range(test_outputs.shape[0]):\n",
        "            real = np.argmax(test_outputs[i])\n",
        "            pred = np.argmax(test_outputs_pred[i])\n",
        "\n",
        "            print(\"Patron %d. Clase real: %d; Clase predicha: %d\" % (i, real, pred))\n",
        "            confusion[real, pred] = confusion[real, pred] + 1\n",
        "\n",
        "        print(\"\")\n",
        "        print(\"Matriz de confusión: \")\n",
        "        print(\"\")\n",
        "        print(confusion)\n",
        "        print(\"\")\n",
        "        \"\"\"\n",
        "\n",
        "    return train_mse, test_mse, train_ccr, test_ccr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-2RA6aOMXJX",
        "colab_type": "text"
      },
      "source": [
        "Función en la cual se realiza una iteración del algoritmo. Se entrena la red y se obtienen los resultados de esta."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkJXD6QG_Gr5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lectura_datos(fichero_train, fichero_test, outputs):\n",
        "    \"\"\" Realiza la lectura de datos.\n",
        "        Recibe los siguientes parámetros:\n",
        "            - fichero_train: nombre del fichero de entrenamiento.\n",
        "            - fichero_test: nombre del fichero de test.\n",
        "            - outputs: número de variables que se tomarán como salidas \n",
        "              (todas al final de la matriz).\n",
        "        Devuelve:\n",
        "            - train_inputs: matriz con las variables de entrada de \n",
        "              entrenamiento.\n",
        "            - train_outputs: matriz con las variables de salida de \n",
        "              entrenamiento.\n",
        "            - test_inputs: matriz con las variables de entrada de \n",
        "              test.\n",
        "            - test_outputs: matriz con las variables de salida de \n",
        "              test.\n",
        "    \"\"\"\n",
        "\n",
        "    #TODO: Completar el código de la función\n",
        "\n",
        "    train = pd.read_csv(fichero_train, names = None)\n",
        "    test = pd.read_csv(fichero_test, names = None)\n",
        "\n",
        "    n_cols = train.shape[1]\n",
        "\n",
        "    train_inputs = train.values[:, 0:n_cols - outputs] #De la primera columna a Cols - Salidas\n",
        "    train_outputs = train.values[:,n_cols - outputs:]  #El resto\n",
        "\n",
        "    test_inputs = test.values[:, 0:n_cols - outputs] #De la primera columna a Cols - Salidas\n",
        "    test_outputs = test.values[:,n_cols - outputs:]  #El resto\n",
        "\n",
        "\n",
        "    return train_inputs, train_outputs, test_inputs, test_outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "med8rHOHiiAn",
        "colab_type": "text"
      },
      "source": [
        "Para poder acceder a las variables de salida, debemos de obtener el número de columnas totales de nuestra base de datos, para restas a este el número de variables de salida (outputs) y acceder a la posición donde estas se encuentran."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmqf1Im0_GzR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def inicializar_centroides_clas(train_inputs, train_outputs, num_rbf):\n",
        "    \"\"\" Inicializa los centroides para el caso de clasificación.\n",
        "        Debe elegir los patrones de forma estratificada, manteniendo\n",
        "        la proporción de patrones por clase.\n",
        "        Recibe los siguientes parámetros:\n",
        "            - train_inputs: matriz con las variables de entrada de \n",
        "              entrenamiento.\n",
        "            - train_outputs: matriz con las variables de salida de \n",
        "              entrenamiento.\n",
        "            - num_rbf: número de neuronas de tipo RBF.\n",
        "        Devuelve:\n",
        "            - centroides: matriz con todos los centroides iniciales\n",
        "                          (num_rbf x num_entradas).\n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "    s = StratifiedShuffleSplit(n_splits=1, test_size=num_rbf, random_state=0)\n",
        "\n",
        "    for train_index, test_index in s.split(train_inputs, train_outputs):\n",
        "        centroides = np.asarray(train_inputs[test_index])\n",
        "\n",
        "    #TODO: Completar el código de la función\n",
        "    return centroides\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uf3QnmUgMiHj",
        "colab_type": "text"
      },
      "source": [
        "Inicializamos los centroides mediante *StratifiedShuffleSplit*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlbfU_7A_jib",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clustering(clasificacion, train_inputs, train_outputs, num_rbf):\n",
        "    \"\"\" Realiza el proceso de clustering. En el caso de la clasificación, se\n",
        "        deben escoger los centroides usando inicializar_centroides_clas()\n",
        "        En el caso de la regresión, se escogen aleatoriamente.\n",
        "        Recibe los siguientes parámetros:\n",
        "            - clasificacion: True si el problema es de clasificacion.\n",
        "            - train_inputs: matriz con las variables de entrada de \n",
        "              entrenamiento.\n",
        "            - train_outputs: matriz con las variables de salida de \n",
        "              entrenamiento.\n",
        "            - num_rbf: número de neuronas de tipo RBF.\n",
        "        Devuelve:\n",
        "            - kmedias: objeto de tipo sklearn.cluster.KMeans ya entrenado.\n",
        "            - distancias: matriz (num_patrones x num_rbf) con la distancia \n",
        "              desde cada patrón hasta cada rbf.\n",
        "            - centros: matriz (num_rbf x num_entradas) con los centroides \n",
        "              obtenidos tras el proceso de clustering.\n",
        "    \"\"\"\n",
        "\n",
        "    #TODO: Completar el código de la función\n",
        "\n",
        "    if clasificacion == True:\n",
        "      centroides = inicializar_centroides_clas(train_inputs, train_outputs, num_rbf)\n",
        "    else:\n",
        "      centroides = 'random'\n",
        "\n",
        "    kmedias = KMeans(n_clusters = num_rbf, init = centroides, n_init = 1, max_iter = 500)\n",
        "    distancias = kmedias.fit_transform(train_inputs)\n",
        "    centros = kmedias.cluster_centers_\n",
        "\n",
        "\n",
        "    return kmedias, distancias, centros"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KAB_z2F3VsA",
        "colab_type": "text"
      },
      "source": [
        "En caso de tratarse de un problema de clasificación, obtenemos los centroides de forma estratidicada, tratando de obtener el mismo número de patrones para cada clase. en cambio, si el problema no es de clasificación, estos centroides se escogeran de forma aletoria.\n",
        "Tras esto, entrenamos (KMeans), obtenemos la matriz de distancias entre los cada patrón y los patrones rbf y, por último, se definen los centroides de cada cluster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UuQrLQ-_joK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calcular_radios(centros, num_rbf):\n",
        "    \"\"\" Calcula el valor de los radios tras el clustering.\n",
        "        Recibe los siguientes parámetros:\n",
        "            - centros: conjunto de centroides.\n",
        "            - num_rbf: número de neuronas de tipo RBF.\n",
        "        Devuelve:\n",
        "            - radios: vector (num_rbf) con el radio de cada RBF.\n",
        "    \"\"\"\n",
        "\n",
        "    #TODO: Completar el código de la función\n",
        "    distancias = pdist(centros)\n",
        "\n",
        "    radios = np.sum(squareform(distancias), 1) / (2 * (num_rbf - 1))\n",
        "\n",
        "    return radios"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIpdt8fC4ZPu",
        "colab_type": "text"
      },
      "source": [
        "El radio para cada uno de los clusters será la mitad de la distancia media a los demás centroides."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOQZ5MdX_jzC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calcular_matriz_r(distancias, radios):\n",
        "    \"\"\" Devuelve el valor de activación de cada neurona para cada patrón \n",
        "        (matriz R en la presentación)\n",
        "        Recibe los siguientes parámetros:\n",
        "            - distancias: matriz (num_patrones x num_rbf) con la distancia \n",
        "              desde cada patrón hasta cada rbf.\n",
        "            - radios: array (num_rbf) con el radio de cada RBF.\n",
        "        Devuelve:\n",
        "            - matriz_r: matriz (num_patrones x (num_rbf+1)) con el valor de \n",
        "              activación (out) de cada RBF para cada patrón. Además, añadimos\n",
        "              al final, en la última columna, un vector con todos los \n",
        "              valores a 1, que actuará como sesgo.\n",
        "    \"\"\"\n",
        "\n",
        "    #TODO: Completar el código de la función\n",
        "\n",
        "    #num_patrones = distancias.shape[0]\n",
        "    sesgo = np.ones((distancias.shape[0], 1))\n",
        "\n",
        "    # Se obtiene la activación de cada RBF para cada patrón.\n",
        "    matriz_r = np.exp(-(np.square(distancias))/(2 * np.square(radios)))\n",
        "\n",
        "    # Se añade la columna de sesgo.\n",
        "    matriz_r = np.append(matriz_r, sesgo, axis=1)\n",
        "    #print(matriz_r)\n",
        "    return matriz_r"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSP4OJJ__j7X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def invertir_matriz_regresion(matriz_r, train_outputs):\n",
        "    \"\"\" Devuelve el vector de coeficientes obtenidos para el caso de la \n",
        "        regresión (matriz beta en las diapositivas)\n",
        "        Recibe los siguientes parámetros:\n",
        "            - matriz_r: matriz (num_patrones x (num_rbf+1)) con el valor de \n",
        "              activación (out) de cada RBF para cada patrón. Además, añadimos\n",
        "              al final, en la última columna, un vector con todos los \n",
        "              valores a 1, que actuará como sesgo.\n",
        "            - train_outputs: matriz con las variables de salida de \n",
        "              entrenamiento.\n",
        "        Devuelve:\n",
        "            - coeficientes: vector (num_rbf+1) con el valor del sesgo y del \n",
        "              coeficiente de salida para cada rbf.\n",
        "    \"\"\"\n",
        "\n",
        "    #TODO: Completar el código de la función\n",
        "\n",
        "    # Se obtiene la pseudo-inversa de la matriz de activación.\n",
        "\n",
        "    trans = np.transpose(matriz_r)\n",
        "\n",
        "    aux1 = np.dot(trans, matriz_r)\n",
        "\n",
        "    aux2 = (np.linalg.pinv(aux1))\n",
        "\n",
        "    p_inv = np.dot(aux2, trans)\n",
        "\n",
        "\n",
        "    #p_inv = np.linalg.pinv(matriz_r)\n",
        "\n",
        "    # Se calculan los coeficientes del modelo.\n",
        "    coeficientes = p_inv.dot(train_outputs)\n",
        "\n",
        "    return coeficientes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihWn-66LM9IX",
        "colab_type": "text"
      },
      "source": [
        "Calculamos la pseudo-inversa y obtenemos los coeficientes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ok5BfteV_kEG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def logreg_clasificacion(matriz_r, train_outputs, eta, l2):\n",
        "    \"\"\" Devuelve el objeto de tipo regresión logística obtenido a partir de la\n",
        "        matriz R.\n",
        "        Recibe los siguientes parámetros:\n",
        "            - matriz_r: matriz (num_patrones x (num_rbf+1)) con el valor de \n",
        "              activación (out) de cada RBF para cada patrón. Además, añadimos\n",
        "              al final, en la última columna, un vector con todos los \n",
        "              valores a 1, que actuará como sesgo.\n",
        "            - train_outputs: matriz con las variables de salida de \n",
        "              entrenamiento.\n",
        "            - eta: valor del parámetro de regularización para la Regresión \n",
        "              Logística.\n",
        "            - l2: True si queremos utilizar L2 para la Regresión Logística. \n",
        "              False si queremos usar L1.\n",
        "        Devuelve:\n",
        "            - logreg: objeto de tipo sklearn.linear_model.LogisticRegression ya\n",
        "              entrenado.\n",
        "    \"\"\"\n",
        "\n",
        "    #TODO: Completar el código de la función\n",
        "\n",
        "    if l2:\n",
        "        reg = 'l2'\n",
        "    else:\n",
        "        reg = 'l1'\n",
        "\n",
        "    logreg = LogisticRegression(penalty = reg, C = 1/eta, fit_intercept = False, solver = 'liblinear', multi_class='auto')\n",
        "    logreg.fit(matriz_r, train_outputs.ravel())\n",
        "    logreg.densify()\n",
        "    a = logreg.coef_\n",
        "    #print(\"COEFICIENTES:  \", np.count_nonzero(a))\n",
        "\n",
        "    return logreg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7WNkXVqNwaS",
        "colab_type": "text"
      },
      "source": [
        "Función mediante el cual se aplica la regresión logísitca a los problemas de clasificación. En esta función se especifica el tipo de regularización (L1 o L2) y es de la cual obtenemos los coeficientes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2d4Ze_r_ySM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(test_file, model_file):\n",
        "    \"\"\" Calcula las predicciones para un conjunto de test que recibe como parámetro. Para ello, utiliza un fichero que\n",
        "    contiene un modelo guardado.\n",
        "    :param test_file: fichero csv (separado por comas) que contiene los datos de test.\n",
        "    :param model_file: fichero de pickle que contiene el modelo guardado.\n",
        "    :return: las predicciones para la variable de salida del conjunto de datos proporcionado.\n",
        "    \"\"\"\n",
        "    test_df = pd.read_csv(test_file, header=None)\n",
        "    test_inputs = test_df.values[:, :]\n",
        "\n",
        "    with open(model_file, 'rb') as f:\n",
        "        saved_data = pickle.load(f)\n",
        "\n",
        "    radios = saved_data['radios']\n",
        "    classification = saved_data['classification']\n",
        "    kmedias = saved_data['kmedias']\n",
        "\n",
        "    test_distancias = kmedias.transform(test_inputs)\n",
        "    test_r = calcular_matriz_r(test_distancias, radios)\n",
        "\n",
        "    if classification:\n",
        "        logreg = saved_data['logreg']\n",
        "        test_predictions = logreg.predict(test_r)\n",
        "    else:\n",
        "        coeficientes = saved_data['coeficientes']\n",
        "        test_predictions = np.dot(test_r, coeficientes)\n",
        "\n",
        "    return test_predictions\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjXkaF7TIWQg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    #Bds = [\"sin.csv\", \"quake.csv\", \"parkinsons.csv\", \"vote.csv\", \"nomnist.csv\"]\n",
        "    Bds = [\"nomnist.csv\"]\n",
        "    salidas = [1]\n",
        "    neuronas = [0.05]\n",
        "    regularizacion = False\n",
        "    eta = [0.01]\n",
        "\n",
        "    for bd in range(1):\n",
        "\n",
        "      train_file = 'train_' + Bds[bd]\n",
        "      test_file = 'test_' + Bds[bd]\n",
        "\n",
        "      regularizacion = False\n",
        "\n",
        "      entrenar_rbf_total(train_file, test_file, True, neuronas[bd], regularizacion, eta[bd], salidas[bd], \"\", False)\n",
        "    #entrenar_rbf_total(train_file, test_file, classification, ratio_rbf, l2, eta, outputs, model_file, pred):"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2yIfrRpOAVP",
        "colab_type": "text"
      },
      "source": [
        "Main del código en el cual se introducen los distintos parámetros a considerar en vectores para realizar la experimentación indicada."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1C3plmHOM-x",
        "colab_type": "text"
      },
      "source": [
        "##Extra\n",
        "\n",
        "Se ha realizado también un pequeño código de prueba, para familializarse con ScikitLearn y con Python, en el cual se aplican los algoritmo de K vecinos más cercanos y regresión logística a la base de datos 'wine'.\n",
        "\n",
        "El código ha sido el siguiente:\n",
        "\n",
        "\n",
        "```\n",
        "#from google.colab import files\n",
        "#files.upload()\n",
        "\n",
        "from sklearn import preprocessing as pp\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import neighbors\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "wine = pd.read_csv('wine.data', names=None)\n",
        "\n",
        "#print(wine)\n",
        "\n",
        "numPatrones = np.random.rand(len(wine)) < 0.6 #Cogemos valores menores a 0.6 y los ponemos como true\n",
        "train = wine[numPatrones]\n",
        "test = wine[~numPatrones]\n",
        "#print(numPatrones)\n",
        "\n",
        "print(train.shape[1])\n",
        "\n",
        "train_inputs = train.values[:,1:13]\n",
        "train_outputs = train.values[:,0]\n",
        "\n",
        "test_inputs = test.values[:,1:13]\n",
        "test_outputs = test.values[:,0]\n",
        "\n",
        "min_max_scaler = pp.MinMaxScaler()\n",
        "X_train_minmax = min_max_scaler.fit_transform(train_inputs)\n",
        "#print(X_train_minmax)\n",
        "\n",
        "X_test_minmax = min_max_scaler.fit_transform(test_inputs)\n",
        "#print(X_test_minmax)\n",
        "\n",
        "for nn in range(1,15):\n",
        "    knn = neighbors.KNeighborsClassifier(n_neighbors=nn)\n",
        "    knn.fit(X_train_minmax, train_outputs)\n",
        "    precisionTrain = knn.score(X_train_minmax, train_outputs)\n",
        "    precisionTest = knn.score(X_test_minmax, test_outputs)\n",
        "    print(\"%d vecinos: \\tCCR train = %.2f%%, \\tCCR test = %.2f%%\" % (nn, precisionTrain*100, precisionTest*100))\n",
        "\n",
        "for nn in range(1,15):\n",
        "    reg = 'l1'\n",
        "\n",
        "    logreg = LogisticRegression(penalty = reg, C = 1/0.01, fit_intercept = False, solver = 'liblinear', multi_class='auto')\n",
        "    logreg.fit(X_train_minmax, train_outputs)\n",
        "    precisionTrain = logreg.score(X_train_minmax, train_outputs)\n",
        "    precisionTest = logreg.score(X_test_minmax, test_outputs)\n",
        "\n",
        "    print(\"%d vecinos: \\tCCR train = %.2f%%, \\tCCR test = %.2f%%\" % (nn, precisionTrain*100, precisionTest*100))\n",
        "```\n",
        "\n",
        "Los resultados obtenidos han sido los siguientes.\n",
        "\n",
        "Para K vecinos más cercanos:\n",
        "\n",
        "![Extra 1](https://drive.google.com/uc?id=1_ENTeqlwcwAxT9ncTo3aBpkwSD5EF7Pi)\n",
        "\n",
        "Para regresión logísitca:\n",
        "\n",
        "![Extra 2](https://drive.google.com/uc?id=1WUiGyMuFe_B6rwKZbZWcM-ERKs3iI0Lf)"
      ]
    }
  ]
}